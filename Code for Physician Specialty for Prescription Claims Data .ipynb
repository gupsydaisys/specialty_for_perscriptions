{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import scipy.optimize as opt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at rows with specialities that are \"physician\"-like\n",
    "included_specialties = ['Internal Medicine', 'Anesthesiology', 'Family Practice', 'Chiropractic',\n",
    "       'Obstetrics/Gynecology', 'Cardiac Surgery',\n",
    "       'Cardiology', 'Dermatology',\n",
    "       'Physical Medicine and Rehabilitation', 'Radiation Oncology',\n",
    "       'Infectious Disease', 'Orthopedic Surgery', 'Endocrinology',\n",
    "       'Urology', 'Emergency Medicine', 'Neurology', 'Nephrology',\n",
    "       'Preventive Medicine', 'Hand Surgery', 'Pulmonary Disease',\n",
    "       'Otolaryngology', 'Plastic and Reconstructive Surgery',\n",
    "       'General Practice', 'Allergy/Immunology', 'Psychiatry',\n",
    "       'Ophthalmology', 'Diagnostic Radiology', 'Psychiatry & Neurology',\n",
    "       'General Surgery', 'Geriatric Medicine',\n",
    "       'Gastroenterology', 'Thoracic Surgery', 'Neuropsychiatry',\n",
    "       'Pain Management', 'Podiatry',\n",
    "       'Hematology/Oncology', 'Optometry', 'Neurosurgery', 'Medical Oncology', 'Surgical Oncology',\n",
    "       'Pediatric Medicine', 'Nuclear Medicine',\n",
    "       'Naturopath', 'Osteopathic Manipulative Medicine',\n",
    "       'Orthopaedic Surgery',\n",
    "       'Family Medicine', 'Rheumatology',\n",
    "       'Vascular Surgery',\n",
    "       'Critical Care (Intensivists)', 'Hospitalist', 'Hematology', 'Maxillofacial Surgery',\n",
    "       'Interventional Pain Management',\n",
    "       'Oral & Maxillofacial Surgery', 'Optician',\n",
    "       'Thoracic Surgery (Cardiothoracic Vascular Surgery)',\n",
    "       'Neurological Surgery', 'Cardiac Electrophysiology',\n",
    "       'Physical Medicine & Rehabilitation', 'Pathology',\n",
    "       'Sports Medicine', 'Sleep Medicine',\n",
    "       'Colorectal Surgery (formerly proctology)', 'Geriatric Psychiatry',\n",
    "       'Addiction Medicine', 'Gynecological/Oncology',\n",
    "       'Interventional Radiology', 'Peripheral Vascular Disease',\n",
    "       'Plastic Surgery',\n",
    "       'Interventional Cardiology', 'Prosthetist',\n",
    "       'Hospice and Palliative Care',\n",
    "       'Neuromusculoskeletal Medicine, Sports Medicine',\n",
    "       'Colon & Rectal Surgery',\n",
    "       'Radiology','Obstetrics & Gynecology',\n",
    "       'Hospital (Dmercs Only)',\n",
    "       'Medical Genetics',\n",
    "       'Clinical Neuropsychologist', 'Naprapath','Pediatrics',\n",
    "       'Audiologist (billing independently)', 'Phlebology']\n",
    "\n",
    "included_set = set(included_specialties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data !! Make sure `data.txt` is in the same folder as this notebook or point to it in the below line !!\n",
    "df = pd.read_csv('data.txt', sep=\"\t\")\n",
    "\n",
    "# Only grab providers who are individuals (as opposed to Organizations)\n",
    "df = df[df.nppes_provider_first_name.notnull()]\n",
    "\n",
    "# Only grab rows with valid PHYSICIAN specialties\n",
    "df = df.loc[df.specialty_description.isin(included_set)]\n",
    "\n",
    "# ^ In the above code, I only reassign to the variable `df` since as an after thought I wanted to remove those rows\n",
    "\n",
    "all_unique_npi = df.npi.unique()\n",
    "# Instead of the above line ^, if you want to use old data uncomment out the below lines\n",
    "# old_X = pd.read_csv('features_combined.txt', sep=\"\\t\")\n",
    "# old_X_without_column_0 = old_X.drop(old_X.columns[[0]], axis=1)\n",
    "# old_y = pd.read_csv('labels_combined.txt', sep=\"\\t\")\n",
    "# old_y_without_column_0 = old_y.drop(old_y.columns[[0]], axis=1)\n",
    "\n",
    "# Grab unique npi\n",
    "# int_version_npi = map(int, old_X_without_column_0.npi.unique())\n",
    "# all_npi = df.npi.unique()\n",
    "# set_of_unproccessed_npi = set(all_npi) - set(int_version_npi)\n",
    "# all_unique_npi = np.asarray(list(set_of_unproccessed_npi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of index <=> specialty for label array creation\n",
    "all_specialties = df.specialty_description.unique()\n",
    "# Instead of the above line ^, if you want to use old data uncomment out the below lines for initial save\n",
    "# and later usage (Do something similar for line `all_generic_names = df.generic_name.unique()`)\n",
    "# all_specialties = pd.DataFrame(df.specialty_description.unique())\n",
    "# all_specialties.to_csv(\"all_specialties.txt\", sep='\\t')\n",
    "# all_specialties = pd.read_csv('all_specialties.txt', sep=\"\\t\")\n",
    "# all_specialties = all_specialties.drop(all_specialties.columns[[0]], axis=1)\n",
    "index_for_specialty = dict((specialty,i) for i, specialty in enumerate(all_specialties))\n",
    "inv_map = {v: k for k, v in index_for_specialty.iteritems()}\n",
    "\n",
    "# Create a map of generic_name => index for feature array creation\n",
    "all_generic_names = df.generic_name.unique()\n",
    "X_header = np.insert(all_generic_names, 0, 'npi')\n",
    "feature_size = X_header.shape[0]\n",
    "generic_name_to_index = {}\n",
    "for i, generic_name in enumerate(X_header):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    generic_name_to_index[generic_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = all_unique_npi[0:40000]\n",
    "# With more time I would have figured out how to make my code run faster + grabbed all npi data, using something like this:\n",
    "# s1 = all_unique_npi[100000:200000]\n",
    "# s2 = all_unique_npi[200000:300000]\n",
    "# s3 = all_unique_npi[300000:400000]\n",
    "# s4 = all_unique_npi[400000:500000]\n",
    "# s5 = all_unique_npi[500000:600000]\n",
    "# s6 = all_unique_npi[600000:700000]\n",
    "# s7 = all_unique_npi[700000:900000]\n",
    "# Learned: You do not get errors if index is too big, interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(columns=X_header)\n",
    "y = np.zeros(s0.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: This code takes a VERY long time to run.  On the order of an hour.\n",
    "for i, npi in enumerate(s0):\n",
    "    if (i % 500) == 0:\n",
    "        print i # To see if anything is actually happening\n",
    "    npi_matches = df.loc[df['npi'] == npi]\n",
    "\n",
    "    speciality = npi_matches.specialty_description.values[0]\n",
    "    y[i] = index_for_specialty[speciality]\n",
    "\n",
    "    # We need to normalize bene_count across doctor's patient panel (as suggested by claims data)\n",
    "    total_bene_counts = npi_matches.bene_count.fillna(5).sum()\n",
    "    feature_vector = np.zeros(feature_size)\n",
    "    feature_vector[0] = npi\n",
    "    for generic_name in npi_matches.generic_name.unique():\n",
    "        generic_name_matches = npi_matches.loc[npi_matches['generic_name'] == generic_name]\n",
    "        index_for_feature_vector = generic_name_to_index[generic_name]\n",
    "        normalized_bene_count = generic_name_matches.bene_count.fillna(5).sum() / total_bene_counts\n",
    "        feature_vector[index_for_feature_vector] = normalized_bene_count\n",
    "    X.loc[i] = feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for later usage without having to rebuild the features + labels\n",
    "# https://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file\n",
    "X.to_csv(\"features_combined.txt\", sep='\\t')\n",
    "pd.DataFrame(y).to_csv(\"labels_combined.txt\", sep='\\t')\n",
    "\n",
    "\n",
    "# If you end up wanting to append data sets together\n",
    "# combined_X = old_X_without_column_0.append(X)\n",
    "# combined_X.to_csv(\"features_combined2.txt\", sep='\\t')\n",
    "\n",
    "# unraveled = old_y_without_column_0.values.ravel()\n",
    "# combined_y = pd.DataFrame(unraveled + y)\n",
    "# combined_y.to_csv(\"labels_combined2.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain= X.head(32000)\n",
    "Xtest = X.tail(8000)\n",
    "\n",
    "Y = pd.DataFrame(y)\n",
    "Ytrain = Y.head(32000)\n",
    "Ytest = Y.tail(8000)\n",
    "\n",
    "Xtrain_without_npi = Xtrain.drop(columns=['npi'])\n",
    "Xtest_without_npi = Xtest.drop(columns=['npi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "model = clf.fit(Xtrain_without_npi, Ytrain.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for testing how reasonable it is that your test set will cover the things you want\n",
    "ytrain_values = set()\n",
    "for e in pd.DataFrame(Ytrain).values:\n",
    "    ytrain_values.add(inv_map[e[0]])\n",
    "    \n",
    "ytest_values = set()\n",
    "for e in pd.DataFrame(Ytest).values:\n",
    "    ytest_values.add(inv_map[e[0]])\n",
    "    \n",
    "len(included_specialties), len(ytrain_values), len(ytest_values), len(ytrain_values.intersection(ytest_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest_predict_proba = model.predict_proba(Xtest_without_npi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest_predict = model.predict(Xtest_without_npi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yravel = Ytest.values.ravel()\n",
    "total_abs = len(yravel)\n",
    "total_good = 0\n",
    "good = set()\n",
    "bad = set()\n",
    "for i, e in enumerate(Ytest_predict):\n",
    "    if e == yravel[i]:\n",
    "        total_good = total_good + 1\n",
    "        good.add(inv_map[e])\n",
    "    bad.add((inv_map[e], inv_map[yravel[i]]))\n",
    "    \n",
    "print total_good, total_abs\n",
    "# print good\n",
    "# print bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain0 = X.head(4500)\n",
    "Xtrain1 = X.tail(5500).head(4500)\n",
    "Xtest = X.tail(5500).tail(1000)\n",
    "\n",
    "Y = pd.DataFrame(y)\n",
    "Ytrain0 = Y.head(4500)\n",
    "Ytrain1 = Y.tail(5500).head(4500)\n",
    "Ytest = Y.tail(5500).tail(1000)\n",
    "\n",
    "Xtrain0_without_npi = Xtrain0.drop(columns=['npi'])\n",
    "Xtrain1_without_npi = Xtrain1.drop(columns=['npi'])\n",
    "Xtest_without_npi = Xtest.drop(columns=['npi'])\n",
    "\n",
    "clf_without_extra_features = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "model = clf_without_extra_features.fit(Xtrain0_without_npi, Ytrain0.values.ravel())\n",
    "\n",
    "Ytrain1_predict_proba = model.predict_proba(Xtrain1_without_npi)\n",
    "\n",
    "Xtrain1_without_npi.shape, pd.DataFrame(Ytrain1_predict_proba).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In [3]:\n",
    "#\n",
    "#df1 = pd.DataFrame([1,2,3], index = np.arange(2000))\n",
    "#df2 = pd.DataFrame([3,5,3], index = np.arange(2000))\n",
    "#pd.concat([df1,df2], axis=1)\n",
    "#Out[3]:\n",
    "#   0  0\n",
    "#2  1  3\n",
    "#3  2  5\n",
    "#4  3  3\n",
    "\n",
    "#size = len(Ytest_predict_proba)\n",
    "#df1 = pd.DataFrame(Ytest_predict_proba, index = np.arange(size))\n",
    "#df2 = pd.DataFrame(Xtest_without_npi, index = np.arange(size))\n",
    "#meow = pd.concat([df1,pd.DataFrame(df2)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
